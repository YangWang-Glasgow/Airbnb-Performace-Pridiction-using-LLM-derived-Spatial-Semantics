{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "# standard library imports\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "# third party imports\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f355ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "#load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d191543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # 4x longer contexts auto supported!\n",
    "    random_state = 7723,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "\n",
    "path = os.path.expanduser('~/OneDrive/LLM//Llama3-70B-full-output.json')\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    llama_results = json.load(f)\n",
    "# llama_results=llama_results[5:10]\n",
    "print(len(llama_results))\n",
    "print(llama_results[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the instructions prompt\n",
    "path = os.path.expanduser('/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/prompts/instruction_template.txt')\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    instruction= f.read()\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = instruction.replace('<location>', 'Edinburgh, UK')\n",
    "instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a88be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data to remove any miscinstructions etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94871331",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Ensure llama_results is iterable\n",
    "if isinstance(llama_results, dict):\n",
    "    llama_results = [llama_results]\n",
    "\n",
    "for result in tqdm(llama_results):\n",
    "    out = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": result.get(\"description\", \"\")\n",
    "    }\n",
    "\n",
    "    nearby = result.get(\"nearby\", {})\n",
    "\n",
    "    # üß† If nearby is a string, try to parse it\n",
    "    if isinstance(nearby, str):\n",
    "        try:\n",
    "            nearby = json.loads(nearby)\n",
    "        except json.JSONDecodeError:\n",
    "            # If parsing fails, treat as empty\n",
    "            nearby = {}\n",
    "\n",
    "    # 1Ô∏è‚É£ Handle missing or empty nearby entries\n",
    "    if not isinstance(nearby, dict) or all(len(v) == 0 for v in nearby.values()):\n",
    "        out[\"response\"] = json.dumps([\n",
    "            {\"specific_locations\": [], \"general_references\": []}\n",
    "        ])\n",
    "        data.append(out)\n",
    "        continue\n",
    "\n",
    "    # 2Ô∏è‚É£ Extract lists safely\n",
    "    specific = nearby.get(\"specific_locations\", [])\n",
    "    general = nearby.get(\"general_locations\", [])\n",
    "    parent  = nearby.get(\"parent_locations\", [])\n",
    "\n",
    "    # Combine general + parent ‚Üí general_references\n",
    "    general_refs = list(set(general + parent))  # optional deduplication\n",
    "\n",
    "    # 3Ô∏è‚É£ Create structured response\n",
    "    structured_response = [\n",
    "        {\n",
    "            \"specific_locations\": specific,\n",
    "            \"general_references\": general_refs,\n",
    "            \"parent_references\": parent\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 4Ô∏è‚É£ Serialize to JSON string\n",
    "    out[\"response\"] = json.dumps(structured_response)\n",
    "    data.append(out)\n",
    "\n",
    "# ‚úÖ Example check\n",
    "print(data[0:5])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the prompt template for model training\n",
    "\n",
    "ft_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides a specfic example which the task should be applied to. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"response\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = ft_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "ft_data = {\"items\":data}\n",
    "print(ft_data['items'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad993558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the test/train split\n",
    "random.seed(7723)\n",
    "trn_idxs = random.sample(range(len(data)), 6500) #2200\n",
    "val_idxs = [x for x in range(len(data)) if x not in trn_idxs]\n",
    "trn_data = [data[i] for i in trn_idxs]\n",
    "val_data = [data[i] for i in val_idxs]\n",
    "\n",
    "trn_dataset = Dataset.from_pandas(pd.DataFrame(trn_data), split=\"train\")\n",
    "print(trn_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd10a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data for input into the model\n",
    "trn_dataset = trn_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da6911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b014359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = trn_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    import torch, gc, os\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(\"GPU memory cleared.\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50156970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08932dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursively_fix_device(module, device):\n",
    "    if not hasattr(module, \"device\") or module.device is None:\n",
    "        module.device = device\n",
    "    for child in module.children():\n",
    "        recursively_fix_device(child, device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "recursively_fix_device(model, device)\n",
    "\n",
    "bad = [n for n, m in model.named_modules() if getattr(m, \"device\", None) is None]\n",
    "print(\"Modules without device:\", bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trian the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce53fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/huggingface_key.txt', 'r') as f:\n",
    "    hf_key = f.read()\n",
    "print(hf_key)\n",
    "# log in to huggingface so the model can be saved\n",
    "login(hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to huggingface\n",
    "model.push_to_hub(\"ywang-gla/ProxiLlama-3.1-8b_trn6500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "from argparse import ArgumentParser\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# third party imports\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/scripts\")\n",
    "\n",
    "from json_utils import validate_json, get_schema #extract_json, \n",
    "\n",
    "\n",
    "class LLM:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name:str, \n",
    "                 prompt_path:str, \n",
    "                 instruct_path:str, \n",
    "                 json_fix_path:str, \n",
    "                 hf_key_path:str):\n",
    "        \n",
    "        self.model, self.tokenizer = self.load_model(model_name)\n",
    "        \n",
    "        self.template = self.load_text(prompt_path)\n",
    "        self.instruction = self.load_text(instruct_path)\n",
    "        self.json_fix_instruction = self.load_text(json_fix_path)\n",
    "        \n",
    "        self.hf_key = self.load_text(hf_key_path)\n",
    "        try:\n",
    "            login(self.hf_key)\n",
    "        except:\n",
    "            raise KeyError('Please ensure HuggingFace key is valid')\n",
    "        \n",
    "    def load_model(self, model_name:str):\n",
    "        \"\"\"Loads the specified modle from Huggingface. Please use a model\n",
    "        hosted on the Unsloth page.\n",
    "        \n",
    "        args:\n",
    "            model_name (str) : path to model on huggingface e.g. \n",
    "            \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "        returns:\n",
    "            FastLanguageModel : unsloth hosted model.\n",
    "            Tokenizer : corresponding tokenizer for model.\n",
    "        \"\"\"\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = 512,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True)\n",
    "        model = FastLanguageModel.for_inference(model)\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def load_text(self, path:str):\n",
    "        \"\"\"Loads data from a txt file\n",
    "        \n",
    "        args:\n",
    "            path (str) : path to the text data.\n",
    "        returns:\n",
    "            str : the text.\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "        \n",
    "    def get_model_response(self, text:str, location:str, max_tokens=512, max_retries=2, curr_retries=0):\n",
    "        \"\"\"Passes the text to the LLM and returns a JSON\n",
    "        args:\n",
    "            text (str) : text to be processed.\n",
    "            location (str) : location relavent to text (e.g. London)\n",
    "            max_new_tokens (int) : max output size for model.\n",
    "            max_retries (int) : max number of times to retry if json is broken\n",
    "            curr_retries (int) : current number of retries performed.  \n",
    "        returns\n",
    "            Json formatted list[dict[str, str]]\n",
    "        \"\"\"\n",
    "        prompt = self.template.format(self.instruction, text, \"\")\n",
    "        prompt = prompt.replace('<location>', location)\n",
    "        inputs = self.tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "        response = self.model.generate(**inputs, max_new_tokens = max_tokens)\n",
    "        output = self.tokenizer.decode(response[0])\n",
    "        # process output and check results\n",
    "        processed = self.process_output(output)\n",
    "        # if retries exceeded, reset and return result\n",
    "        #sys.stdout.write(f'Current Retries = {curr_retries}')\n",
    "        if curr_retries > max_retries:\n",
    "            logger.info('Max Retries Exceeded! Attempting LLM fix.')\n",
    "            processed = self.llm_json_fix(output)\n",
    "            logger.info(processed)\n",
    "            curr_retries=0\n",
    "            return processed\n",
    "        # if an empty list, return as normal\n",
    "        elif len(processed)==0:\n",
    "            curr_retries = 0\n",
    "            return processed\n",
    "        # if ['invalid json'] or ['misconstructed json'] retry\n",
    "        elif isinstance(processed[0], str):\n",
    "            curr_retries += 1\n",
    "            max_tokens += 256\n",
    "            logger.info(f'Current Retries = {curr_retries}')\n",
    "            logger.info(f'Max Tokens = {max_tokens}')\n",
    "            logger.info(processed[0])\n",
    "            \n",
    "            \n",
    "            processed = self.get_model_response(text, location, max_tokens, max_retries, curr_retries)\n",
    "            return processed\n",
    "        # otherwise we will be ok to return the processed output\n",
    "        else:\n",
    "            curr_retries = 0\n",
    "            return processed\n",
    "        \n",
    "    def process_output(self, output:str)->list[dict[str,str]]:\n",
    "        \"\"\"Takes a string output from the LLM, extracts the relavent JSON and \n",
    "        processes it with reference to the schema defined in 'json_utils'\n",
    "        \n",
    "        args: \n",
    "            output (str) : a string containing a (possibly misconstructed) json.\n",
    "        \n",
    "        returns:\n",
    "            list[dict[str,str]] : A valid json in accordance with the shema. \n",
    "        \n",
    "        notes:\n",
    "            Returns ['misconstructed json'] or ['invalid json'] if the json \n",
    "            could not be constructed or validated, respectively.\n",
    "        \"\"\"\n",
    "        # get the response part of the output\n",
    "        response = output.split('### Response:')[1:][0]\n",
    "        # load schema\n",
    "        schema=get_schema()\n",
    "        # extract and validate\n",
    "        try:\n",
    "            json_out = extract_json(response)\n",
    "        except:\n",
    "            return ['misconstructed json']\n",
    "        try:\n",
    "            valid_json_out = validate_json(json_out, schema) \n",
    "        except:\n",
    "            return ['invalid json']\n",
    "        return valid_json_out\n",
    "    \n",
    "    def llm_json_fix(self, output):\n",
    "        \"\"\"As a last resort, we can ask the LLM to fix a JSON it has produced. \n",
    "        \"\"\"\n",
    "        json_str = output.split('### Response:')[1:][0]\n",
    "            \n",
    "        prompt = self.template.format(self.json_fix_instruction, json_str, '')\n",
    "        inputs = self.tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "        response = self.model.generate(**inputs, max_new_tokens = 1024)\n",
    "        output = self.tokenizer.decode(response[0])\n",
    "        # process output and check results\n",
    "        processed = self.process_output(output)\n",
    "        return processed        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70683089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    import torch, gc, os\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(\"GPU memory cleared.\")\n",
    "\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(model_name='ywang-gla/ProxiLlama-3.1-8b_trn6500',\n",
    "            prompt_path='/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/prompts/prompt_template.txt',\n",
    "            instruct_path='/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/prompts/instruction_template.txt',\n",
    "            json_fix_path='/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/prompts/json_fix_instruction.txt',\n",
    "            hf_key_path='/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/huggingface_key.txt')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "outputs = []\n",
    "for d in tqdm(val_data[0:10]):\n",
    "  \n",
    "  tst_outputs = model.get_model_response(text=d['input'],\n",
    "                                         location='Edinburgh, UK',\n",
    "                                         max_tokens=512, \n",
    "                                         max_retries=2)\n",
    "  outputs.append(tst_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save (dump) to a JSON file\n",
    "with open(r'/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/results/test_validation_output_from8b.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(outputs, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "val_data[i]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc335d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try on test set\n",
    "test_df = pd.read_csv('large_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i, row in tqdm(test_df.iterrows(), total=100):\n",
    "  text = row['description']\n",
    "  loc = row['location']\n",
    "  tst_outputs = model.get_model_response(text=text,\n",
    "                                         location=loc,\n",
    "                                         max_tokens=512, \n",
    "                                         max_retries=2)\n",
    "  outputs.append(tst_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "print(test_df.iloc[i].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a78df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "def extract_locations_from_json(json_path, output_csv=\"extracted_locations.csv\"):\n",
    "    \"\"\"\n",
    "    Extract specific_locations, general_locations, and parent_locations from\n",
    "    the 'nearby' field of model-generated JSON objects.\n",
    "\n",
    "    Handles multiple field naming variants and delimiters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load JSON\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        key = entry.get(\"key\")\n",
    "        desc = entry.get(\"description\", \"\")\n",
    "        nearby = entry.get(\"nearby\", \"\")\n",
    "\n",
    "        # --- Step 1. Find the block after 'Response:' and before 'Reasoning' or '<|eot_id|>'\n",
    "        match = re.search(\n",
    "            r'(?:Response:|### Response:|Here is the output in JSON format:)(.*?)(?:Reasoning|<\\|eot_id\\|>|$)',\n",
    "            nearby,\n",
    "            flags=re.S | re.I\n",
    "        )\n",
    "\n",
    "        if not match:\n",
    "            rows.append({\n",
    "                \"key\": key,\n",
    "                \"description\": desc,\n",
    "                \"specific_locations\": [],\n",
    "                \"general_locations\": [],\n",
    "                \"parent_locations\": []\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        block = match.group(1)\n",
    "\n",
    "        # --- Step 2. Clean up the block to resemble JSON\n",
    "        block = block.replace(\"<br />\", \"\").replace(\"\\n\", \" \").replace(\"->\", \":\")\n",
    "        block = re.sub(r'\"\"\"|‚Äú|‚Äù', '\"', block)\n",
    "        block = re.sub(r'(\\b(references|locations)\\b)', r'\\1', block)\n",
    "\n",
    "        # Normalize field names\n",
    "        replacements = {\n",
    "            \"specific_references\": \"specific_locations\",\n",
    "            \"general_references\": \"general_locations\",\n",
    "            \"parent_references\": \"parent_locations\"\n",
    "        }\n",
    "        for old, new in replacements.items():\n",
    "            block = block.replace(old, new)\n",
    "\n",
    "        # --- Step 3. Extract lists using regex (fallback if block isn't valid JSON)\n",
    "        def extract_list(field):\n",
    "            pattern = rf'\"{field}\"\\s*:\\s*\\[([^\\]]*)\\]'\n",
    "            found = re.search(pattern, block)\n",
    "            if not found:\n",
    "                return []\n",
    "            try:\n",
    "                return [i.strip().strip('\"\\' ') for i in found.group(1).split(\",\") if i.strip()]\n",
    "            except Exception:\n",
    "                return []\n",
    "\n",
    "        specific = extract_list(\"specific_locations\")\n",
    "        general = extract_list(\"general_locations\")\n",
    "        parent = extract_list(\"parent_locations\")\n",
    "\n",
    "        # --- Step 4. Save results\n",
    "        rows.append({\n",
    "            \"key\": key,\n",
    "            \"description\": desc,\n",
    "            \"specific_locations\": specific,\n",
    "            \"general_locations\": general,\n",
    "            \"parent_locations\": parent\n",
    "        })\n",
    "\n",
    "    # --- Step 5. Export to CSV\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úÖ Extracted {len(df)} entries to {output_csv}\")\n",
    "    return df\n",
    "\n",
    "extract_locations_from_json(\n",
    "    \"/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/results/llama8b_description_homeNoTraining_output_atmp1.json\",\n",
    "    \"/home/yw30f/OneDrive/LLM/code/UBDC_proximity/UBDC_proximity/results/llama8b_description_roomNoTraining_output_atmp1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
